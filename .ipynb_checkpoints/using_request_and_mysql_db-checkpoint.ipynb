{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4de6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 12:27:12.513636: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-06 12:27:12.692554: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-06 12:27:12.693965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 12:27:14.060089: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from crontab import CronTab\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import select, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "# Add path of subdirectory containing own modules\n",
    "modules_path = os.path.join(os.getcwd(), 'data_collect_app')\n",
    "if modules_path not in sys.path:\n",
    "    sys.path.append(modules_path)\n",
    "\n",
    "import finrail_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de0a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates tables in finrail db, returns database engine\n",
    "engine = finrail_db.create_tables(db_str='mysql+mysqlconnector://root:admin123@localhost:5000/finrail')\n",
    "# Define class, bind to engine\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90fda68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added data of date: 2015-12-12\n",
      "Added data of date: 2015-12-13\n",
      "Added data of date: 2015-12-14\n",
      "Added data of date: 2015-12-15\n",
      "Added data of date: 2015-12-16\n",
      "Added data of date: 2015-12-17\n"
     ]
    }
   ],
   "source": [
    "finrail_db.add_compositions(s=session, date_end=dt.date(2015, 12, 18), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a609f741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2020, 6, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL statement to be executed\n",
    "\n",
    "#    WITH Y AS ( # CTE Y contains columns: departure date, length of all wagons in a train (if more than 1 journey_section\n",
    "#    # in a train run: maximum length among all journey sections is selected\n",
    "#        SELECT T.dep_date AS dep_date, MAX(Z.length) AS length\n",
    "#        FROM trains AS T\n",
    "#        LEFT JOIN ( # Left join will will provide all journey sections for each train\n",
    "#            SELECT J.train_id AS id, SUM(W.length) / 100 AS length #aggregate sum of length of all wagons (in meters)\n",
    "#            # in a journey section\n",
    "#            FROM journey_section AS J\n",
    "#            LEFT JOIN wagon as W ON W.journey_id = J.id # Left join will provide all wagons in each journey section\n",
    "#            GROUP BY J.id # group by id of journey section will cause aggregation to run over each journey section\n",
    "#            ) AS Z # name Z for table with columns id: (id of every train), \n",
    "#            # length (sum of length of all wagons in a journey section)\n",
    "#        ON T.id = Z.id\n",
    "#        GROUP BY T.id # Group by id of table trains will cause MAX(Z.length) to provide length of all wagons from \n",
    "#        # journey section with longest train composition\n",
    "#        )\n",
    "#    SELECT Y.dep_date, SUM(Y.length) # Aggregate length of all wagons for each day\n",
    "#    FROM Y\n",
    "#    GROUP BY Y.dep_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9c33d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tweak_train(df_):\n",
    "    '''Function takes DataFrame as returned from SQL-query and returns processed DataFrame\n",
    "    Transformations:\n",
    "        - DataType: update to all columns\n",
    "        - Introducing columns \"commuter\" and \"long_distance\" by grouping by date and train category\n",
    "          and then unstacking ones\n",
    "        - pushing the date information from index to own column\n",
    "        - Renaming and setting back nested column names\n",
    "        \n",
    "    '''\n",
    "    return (df_\n",
    "    .astype({\n",
    "        'date': 'datetime64',\n",
    "        'train_cat': 'category',\n",
    "        'total_length': np.float32\n",
    "    })\n",
    "    .groupby(['date', 'train_cat'])\n",
    "    .max().unstack()\n",
    "    .reset_index()\n",
    "    .set_axis(['date', 'commuter', 'long_distance'], axis=1)\n",
    "           )\n",
    "\n",
    "# Open fire and read stored SQL query to variable\n",
    "with open('sql_query.txt', 'r') as w:\n",
    "    sql_query_str = w.read()\n",
    "    \n",
    "# Open SQL connection and send query. This query will:\n",
    "# 1. Sum length of all wagon in a journey section\n",
    "# 2. Choose maximum length of all wagons among journey sections for each train\n",
    "# 3. Sum length of wagons for all trains per day, grouped by train category (Commuter, Long-distance)\n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql_query(text(sql_query_str), connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "58ac43f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>train_cat</th>\n",
       "      <th>total_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-12</td>\n",
       "      <td>Commuter</td>\n",
       "      <td>44320.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-12</td>\n",
       "      <td>Long-distance</td>\n",
       "      <td>25543.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>Commuter</td>\n",
       "      <td>34981.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>Long-distance</td>\n",
       "      <td>24279.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-14</td>\n",
       "      <td>Commuter</td>\n",
       "      <td>80536.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6009</th>\n",
       "      <td>2024-03-03</td>\n",
       "      <td>Long-distance</td>\n",
       "      <td>21241.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>2024-03-04</td>\n",
       "      <td>Commuter</td>\n",
       "      <td>91406.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6011</th>\n",
       "      <td>2024-03-04</td>\n",
       "      <td>Long-distance</td>\n",
       "      <td>24124.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6012</th>\n",
       "      <td>2024-03-05</td>\n",
       "      <td>Commuter</td>\n",
       "      <td>91896.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6013</th>\n",
       "      <td>2024-03-05</td>\n",
       "      <td>Long-distance</td>\n",
       "      <td>23306.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6014 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date      train_cat  total_length\n",
       "0     2015-12-12       Commuter      44320.80\n",
       "1     2015-12-12  Long-distance      25543.22\n",
       "2     2015-12-13       Commuter      34981.80\n",
       "3     2015-12-13  Long-distance      24279.37\n",
       "4     2015-12-14       Commuter      80536.80\n",
       "...          ...            ...           ...\n",
       "6009  2024-03-03  Long-distance      21241.48\n",
       "6010  2024-03-04       Commuter      91406.40\n",
       "6011  2024-03-04  Long-distance      24124.32\n",
       "6012  2024-03-05       Commuter      91896.00\n",
       "6013  2024-03-05  Long-distance      23306.16\n",
       "\n",
       "[6014 rows x 3 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb08babf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date             datetime64[ns]\n",
       "commuter                float32\n",
       "long_distance           float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = tweak_train(df)\n",
    "df.max()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d79e28fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "commuter_train = tf.data.Dataset.from_tensor_slices(df['commuter'][:1847].values) #training set until\n",
    "                                                                                  # 2020 including\n",
    "\n",
    "def timeseries_window(data, seq_length, shift=1, stride=1):\n",
    "    '''Function takes dataset and returns dataset containing windows with data from input dataset.\n",
    "    Parameters:\n",
    "        data <tf.data.Dataset> input dataset\n",
    "        seq_length <int> defines length of windows in output dataset\n",
    "        shift <int> defines how many time steps of gap are between two consecutive windows\n",
    "        stride <int> defines how many time steps are between two consecutive output data points\n",
    "        \n",
    "    Return:\n",
    "        <tf.data.Dataset> Dataset containing windows of seq_length based on input dataset data\n",
    "    '''\n",
    "    data = data.window(size=seq_length, shift=shift, stride=stride, drop_remainder=True)\n",
    "    data = data.flat_map(lambda x: x) # flatten nested Dataset structure returned by .window()\n",
    "    return data.batch(seq_length) # batch of size seq_length will give one window in each batch\n",
    "\n",
    "def timeseries_dataset_seq2seq(data, forecast_length=1, seq_length=7):\n",
    "    '''Function takes Dataset and returns Dataset with windows suitable to train a \n",
    "    sequence to sequence RNN\n",
    "    Parameters:\n",
    "        data <tf.data.Dataset> input dataset\n",
    "        forecast_length <int> number of time steps to be forecasted into the future\n",
    "        seq_length <int> length of sequences fed to RNN (number of consecutive time steps \n",
    "        in one training instance)\n",
    "    '''\n",
    "    data = timeseries_window(data, forecast_length+1) # First dimension one time step longer than\n",
    "                                                      # forecast_length, as targets are generated as well\n",
    "    data = timeseries_window(data, seq_length) # Second dimension consists of windows of size sequence length\n",
    "    return data.map(lambda x: (x[:, 0], x[:, 1:])) # map to tuple (training instance, target)\n",
    "\n",
    "\n",
    "commuter_train = timeseries_dataset_seq2seq(commuter_train, 14, 30)\n",
    "commuter_train = commuter_train.shuffle(10*32, seed=42)\n",
    "commuter_train = commuter_train.cache()\n",
    "commuter_train = commuter_train.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0f5bf3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "57/57 [==============================] - 3s 12ms/step - loss: 2333501952.0000\n",
      "Epoch 2/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 717251008.0000\n",
      "Epoch 3/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 605197504.0000\n",
      "Epoch 4/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 544520576.0000\n",
      "Epoch 5/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 503427616.0000\n",
      "Epoch 6/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 473272832.0000\n",
      "Epoch 7/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 449571424.0000\n",
      "Epoch 8/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 429604864.0000\n",
      "Epoch 9/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 410698880.0000\n",
      "Epoch 10/50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 386928512.0000\n",
      "Epoch 11/50\n",
      "57/57 [==============================] - 1s 12ms/step - loss: 346877248.0000\n",
      "Epoch 12/50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 301106528.0000\n",
      "Epoch 13/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 273814080.0000\n",
      "Epoch 14/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 258480816.0000\n",
      "Epoch 15/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 247784656.0000\n",
      "Epoch 16/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 239135280.0000\n",
      "Epoch 17/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 231619104.0000\n",
      "Epoch 18/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 224837712.0000\n",
      "Epoch 19/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 218526864.0000\n",
      "Epoch 20/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 212406240.0000\n",
      "Epoch 21/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 206112320.0000\n",
      "Epoch 22/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 199194640.0000\n",
      "Epoch 23/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 191224096.0000\n",
      "Epoch 24/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 182110000.0000\n",
      "Epoch 25/50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 172447504.0000\n",
      "Epoch 26/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 163343520.0000\n",
      "Epoch 27/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 155737536.0000\n",
      "Epoch 28/50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 149939840.0000\n",
      "Epoch 29/50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 145703856.0000\n",
      "Epoch 30/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 142584672.0000\n",
      "Epoch 31/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 140196624.0000\n",
      "Epoch 32/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 138273488.0000\n",
      "Epoch 33/50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 136646016.0000\n",
      "Epoch 34/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 135210112.0000\n",
      "Epoch 35/50\n",
      "57/57 [==============================] - 1s 13ms/step - loss: 133902240.0000\n",
      "Epoch 36/50\n",
      "57/57 [==============================] - 1s 13ms/step - loss: 132682328.0000\n",
      "Epoch 37/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 131523624.0000\n",
      "Epoch 38/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 130407256.0000\n",
      "Epoch 39/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 129319640.0000\n",
      "Epoch 40/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 128251192.0000\n",
      "Epoch 41/50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 127195304.0000\n",
      "Epoch 42/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 126147784.0000\n",
      "Epoch 43/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 125106496.0000\n",
      "Epoch 44/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 124070880.0000\n",
      "Epoch 45/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 123042184.0000\n",
      "Epoch 46/50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 122023536.0000\n",
      "Epoch 47/50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 121020360.0000\n",
      "Epoch 48/50\n",
      "57/57 [==============================] - 1s 10ms/step - loss: 120040504.0000\n",
      "Epoch 49/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 119093296.0000\n",
      "Epoch 50/50\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 118188848.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5fdf197d30>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_seq2seq = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.Normalization(mean=0, variance=1E10),\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.Dense(14, activation='linear'),\n",
    "    tf.keras.layers.Normalization(mean=0, variance=1E-10)\n",
    "])\n",
    "rnn_seq2seq.compile(optimizer='adam', loss='mse')\n",
    "rnn_seq2seq.fit(commuter_train, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d6eef1aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/layers/rnn/lstm.py\", line 616, in call\n        timesteps = input_shape[0] if self.time_major else input_shape[1]\n\n    TypeError: Exception encountered when calling layer 'lstm_19' (type LSTM).\n    \n    'NoneType' object is not subscriptable\n    \n    Call arguments received by layer 'lstm_19' (type LSTM):\n      • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n      • mask=None\n      • training=False\n      • initial_state=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_seq2seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#result[1, -1, :].round(1)\u001b[39;00m\n",
      "File \u001b[0;32m~/ml_for_physicists/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileww4djni_.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/felbus/ml_for_physicists/lib/python3.10/site-packages/keras/src/layers/rnn/lstm.py\", line 616, in call\n        timesteps = input_shape[0] if self.time_major else input_shape[1]\n\n    TypeError: Exception encountered when calling layer 'lstm_19' (type LSTM).\n    \n    'NoneType' object is not subscriptable\n    \n    Call arguments received by layer 'lstm_19' (type LSTM):\n      • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n      • mask=None\n      • training=False\n      • initial_state=None\n"
     ]
    }
   ],
   "source": [
    "result = rnn_seq2seq.predict(a)\n",
    "#result[1, -1, :].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "347338a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[83794.  83746.  83440.  83844.6 51354.4 39663.2 84129.6 83566.8 84215.8\n",
      " 83635.6 82770.8 51688.8 39813.6 83602.8], shape=(14,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i, (val, target) in enumerate(commuter_train.take(1)):\n",
    "    print(target[1, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4e2f8f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 30, 1])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.Tensor()\n",
    "for i, (train, target) in enumerate(commuter_train):\n",
    "    a = train\n",
    "    if i > 2:\n",
    "        break\n",
    "a = a[0, :]\n",
    "a = a[np.newaxis, :, np.newaxis]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2ee27c26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a, b \u001b[38;5;241m=\u001b[39m commuter_train\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "a, b = commuter_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e215421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 2s 3ms/step - loss: 0.3161\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3099\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3039\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2979\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2919\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2859\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2796\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2733\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2668\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f5fb48ef010>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.LSTM(3, return_sequences=True)    \n",
    "])\n",
    "test_rnn.compile(loss='mse', optimizer='adam')\n",
    "x_training_data = np.random.rand(500, 1)\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_training_data)\n",
    "#x_train = train.batch(32)\n",
    "y_training_data = np.random.rand(500, 3)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_training_data)\n",
    "#y_train = y_train.batch(32)\n",
    "train = ()\n",
    "test_rnn.fit(x=x_training_data, y=y_training_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0bf8dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.09106699, 0.04451869, 0.08681474],\n",
       "        [0.2200988 , 0.06532016, 0.21933918],\n",
       "        [0.24100545, 0.07557128, 0.27738008],\n",
       "        [0.30475822, 0.07128187, 0.33445764],\n",
       "        [0.31270587, 0.07095621, 0.36696607],\n",
       "        [0.33547252, 0.06462032, 0.3862334 ],\n",
       "        [0.30197963, 0.06427979, 0.39682716],\n",
       "        [0.322113  , 0.05587882, 0.3969907 ],\n",
       "        [0.3415638 , 0.05122953, 0.40391538],\n",
       "        [0.3030132 , 0.0515345 , 0.41118127]]], dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rnn.predict(np.random.rand(1, 10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25804b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.InputLayer(input_shape=(None))\n",
    "scaling_layer = tf.keras.layers.Normalization(mean=0, variance=1E5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae57b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This blocks evaluates all possible keys in the nested dictionary \"wagon\" in compositions of one day\n",
    "\n",
    "properties_dict = dict()\n",
    "for train in k.json():\n",
    "    for journey in (train['journeySections']):\n",
    "        for wagon in journey['wagons']:\n",
    "            for i, prop in enumerate(wagon.keys()):\n",
    "                try:\n",
    "                    properties_dict[prop]\n",
    "                except:\n",
    "                    properties_dict[prop] = prop\n",
    "print(properties_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6376e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84644132",
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "session.add(bsp)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e2ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'w') as w:\n",
    "    w.write('haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c57c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
