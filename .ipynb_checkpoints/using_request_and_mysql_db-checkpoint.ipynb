{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4de6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 10:52:17.796063: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-13 10:52:18.228671: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-13 10:52:18.230579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-13 10:52:19.651054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from crontab import CronTab\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sqlalchemy import select, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from time import strftime\n",
    "import timeit\n",
    "\n",
    "# Add path of subdirectory containing own modules\n",
    "modules_path = os.path.join(os.getcwd(), 'data_collect_app')\n",
    "if modules_path not in sys.path:\n",
    "    sys.path.append(modules_path)\n",
    "\n",
    "import finrail_db\n",
    "\n",
    "# Load tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Set random seed for reproduceability\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dceda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory for tensorboard log files\n",
    "def dir_logs(parent_dir='tf_log'):\n",
    "    return Path(parent_dir) / strftime('%Y_%m_%d_%H_%M_%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9c33d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tweak_train(df_):\n",
    "    '''Function takes DataFrame as returned from SQL-query and returns processed DataFrame\n",
    "    Transformations:\n",
    "        - DataType: update to all columns\n",
    "        - Introducing columns \"commuter\" and \"long_distance\" by grouping by date and train category\n",
    "          and then unstacking ones\n",
    "        - pushing the date information from index to own column\n",
    "        - Renaming and setting back nested column names\n",
    "        \n",
    "    '''\n",
    "    return (df_\n",
    "    .astype({\n",
    "        'date': 'datetime64',\n",
    "        'train_cat': 'category', # set as category because of low cardenality\n",
    "        'total_length': np.float32 # float32 as used later in tensorflow\n",
    "    })\n",
    "    .groupby(['date', 'train_cat']) # grouping twice, so \"train_cat\" can be unstacked later\n",
    "    .max().unstack()\n",
    "    .reset_index() # to have dates in own column\n",
    "    .set_axis(['date', 'commuter', 'long_distance'], axis=1) # set column names, flatten nested column index\n",
    ")\n",
    "# Creates tables in finrail db, returns database engine\n",
    "engine = finrail_db.create_tables(db_str='mysql+mysqlconnector://root:admin123@localhost:5000/finrail')\n",
    "\n",
    "# Open fire and read stored SQL query to variable\n",
    "with open('sql_query.txt', 'r') as w:\n",
    "    sql_query_str = w.read()\n",
    "    \n",
    "# Open SQL connection and send query. This query will:\n",
    "# 1. Sum length of all wagon in a journey section\n",
    "# 2. Choose maximum length of all wagons among journey sections for each train\n",
    "# 3. Sum length of wagons for all trains per day, grouped by train category (Commuter, Long-distance)\n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql_query(text(sql_query_str), connection)\n",
    "\n",
    "# Apply tweak_train to output of SQL query to obtain desired time series\n",
    "df = tweak_train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7db9dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Metric(tf.keras.metrics.Metric):\n",
    "    '''Metric calculating the root mean squared error (RMSE) for a sequence to sequence recurrent\n",
    "    neuronal network (RNN) exclusively based on the last predicted vector of a sequence. \n",
    "    This is useful in situation, where a sequence to sequence RNN is trained, but for production \n",
    "    only the last predicted vector matters. This occurs for example in time series prediction.\n",
    "    This metric allows to evaluate the model performance in time series prediction exclusivley on\n",
    "    the parts of output that matters for production. Instead the loss of a sequence to sequence \n",
    "    model training takes all predicted vectors along a sequence into account.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, time_series_index=None, **kwarg):\n",
    "        '''Function hands over kwargs to parent class and initiates two weights, which will \n",
    "        hold the sum of squares and the total count of summed numbers.\n",
    "        Parameters:\n",
    "            time_series_index <int> If used with a model, that outputs more than one time series, \n",
    "            specify index of time series for which custom metric value should be calculated\n",
    "        '''\n",
    "        super().__init__(**kwarg) # pass kwargs to parent class\n",
    "        self.time_series_index = time_series_index #index of time series if multivariate forecast\n",
    "        self.sum_of_squares = self.add_weight('sum_of_squares', initializer='zeros')\n",
    "        self.sample_count = self.add_weight('sample_count', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ''' Function will add to sum_of_squares and sample_counts every batch.'''\n",
    "        if self.time_series_index == None:\n",
    "            # True, if class is in use for forecasting single time series\n",
    "            # sum up how many data point in batch will be summed            \n",
    "            self.sample_count.assign_add(tf.cast(tf.size(y_pred[:, -1, :]), tf.float32))\n",
    "            # sum of squares of difference of y_true and y_pred on last sequence\n",
    "            self.sum_of_squares.assign_add(tf.reduce_sum(\n",
    "                tf.math.square(y_pred[:, -1, :] - y_true[:, -1, :]))\n",
    "            )\n",
    "        else:\n",
    "            # If class is in use for multivariate forecasting, calculate for selected time series only\n",
    "            # sum up how many data point in batch will be summed\n",
    "            self.sample_count.assign_add(tf.cast(\n",
    "                tf.size(y_pred[:, -1, :, self.time_series_index]), tf.float32)\n",
    "            )\n",
    "            # sum of squares of difference of y_true and y_pred on last sequence\n",
    "            self.sum_of_squares.assign_add(tf.reduce_sum(tf.math.square(\n",
    "                y_true[:, -1, :, self.time_series_index] - y_pred[:, -1, :, self.time_series_index]\n",
    "            )))\n",
    "    \n",
    "    def result(self):\n",
    "        '''Function will calculate the RMSE at the end of every epoch'''\n",
    "        return tf.math.sqrt(self.sum_of_squares / self.sample_count)\n",
    "                                    \n",
    "    def reset_state(self):\n",
    "        '''Function will reset all stateful variables to zero'''\n",
    "        self.sample_count.assign(0)\n",
    "        self.sum_of_squares.assign(0)\n",
    "        \n",
    "    def get_config(self):\n",
    "        '''Function will overwrite get_config() of parent class to include \"time_series_index\"'''\n",
    "        conf_dict = super().get_config()\n",
    "        return {**conf_dict, 'time_series_index': self.time_series_index}\n",
    "        \n",
    "def timeseries_window(data, seq_length, shift=1, stride=1):\n",
    "    '''Function takes dataset and returns dataset containing windows with data from input dataset.\n",
    "    Parameters:\n",
    "        data <tf.data.Dataset> input dataset\n",
    "        seq_length <int> defines length of windows in output dataset\n",
    "        shift <int> defines how many time steps of gap are between two consecutive windows\n",
    "        stride <int> defines how many time steps are between two consecutive output data points\n",
    "        \n",
    "    Return:\n",
    "        <tf.data.Dataset> Dataset containing windows of seq_length based on input dataset data\n",
    "    '''\n",
    "    data = data.window(size=seq_length, shift=shift, stride=stride, drop_remainder=True)\n",
    "    data = data.flat_map(lambda x: x) # flatten nested Dataset structure returned by .window()\n",
    "    return data.batch(seq_length) # batch of size seq_length will give one window in each batch\n",
    "\n",
    "def timeseries_dataset_seq2seq(data, forecast_length=1, seq_length=7):\n",
    "    '''Function takes Dataset and returns Dataset with windows suitable to train a \n",
    "    sequence to sequence RNN\n",
    "    Parameters:\n",
    "        data <tf.data.Dataset> input dataset\n",
    "        forecast_length <int> number of time steps to be forecasted into the future\n",
    "        seq_length <int> length of sequences fed to RNN (number of consecutive time steps \n",
    "        in one training instance)\n",
    "    '''\n",
    "    data = timeseries_window(data, forecast_length+1) # First dimension one time step longer than\n",
    "                                                      # forecast_length, as targets are generated as well\n",
    "    data = timeseries_window(data, seq_length) # Second dimension consists of windows of size sequence length\n",
    "    # map to tuple (training instance, target)\n",
    "    return data.map(lambda x: (x[:, 0], x[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def prepare_training_dataset(df_, column, row_split, forecast_length=14, seq_length=30, \n",
    "                             batch_size=32, seed=42, reshuffle_each_iteration=True):\n",
    "    '''Function takes Dataframe and returns tf.data.Dataset with specs:\n",
    "    Parameters:\n",
    "        df_ <pd.Dataframe> Dataframe with time series data (np.float32) in columns\n",
    "        column <string> name of column or list of column names in DataFrame to use\n",
    "        row_split <tuple of two int> defines row index between data is extracted from df_\n",
    "        forecast_length <int> number of time steps to be forecasted into the future\n",
    "        seq_length <int> length of sequences fed to RNN (number of consecutive time steps \n",
    "        batch_size <int> batch_size of returned Dataset\n",
    "        seed <int> random seed for shuffling data\n",
    "        reshuffle_each_iteration <boolean> Defines wheater Dataset is ot be reshuffled after each\n",
    "        training epoch\n",
    "    Return:\n",
    "        <tf.data.Dataset> ready to feed to .fit() of an sequence to sequence RNN\n",
    "    '''\n",
    "    data = tf.data.Dataset.from_tensor_slices(df[column][:1847].values / 1E5)\n",
    "    data = timeseries_dataset_seq2seq(data, forecast_length, seq_length)\n",
    "    data = data.cache() # cache, so that previous transformation are only performed ones\n",
    "    data = data.shuffle(500, seed=seed, reshuffle_each_iteration=reshuffle_each_iteration)\n",
    "    return data.batch(batch_size=batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#training set until 2020 including\n",
    "timeseries_train = prepare_training_dataset(df, ['commuter', 'long_distance'], (0, 1847), seq_length=56)\n",
    "\n",
    "#validation set from 2021 to 2022 including\n",
    "timeseries_val = prepare_training_dataset(df, ['commuter', 'long_distance'], (1847, 2577), \n",
    "                                        batch_size=500, reshuffle_each_iteration=False, seq_length=56)\n",
    "#test set from 2023 to 2024-03-06\n",
    "timeseries_test = prepare_training_dataset(df, ['commuter', 'long_distance'], (2577, 3008), \n",
    "                                        batch_size=500, reshuffle_each_iteration=False, seq_length=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0f5bf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer stack that defines input shape and will scale down inputs by a factor of 1E5\n",
    "input_processing = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.Rescaling(1E-5)\n",
    "])\n",
    "\n",
    "#Output layer that will scale up predictions by a factor of 1E5\n",
    "output_processing = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1E5)\n",
    "])\n",
    "\n",
    "#RNN laer stack for a sequence to sequence model for univariate time series\n",
    "rnn_seq2seq = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.Dense(28, activation='linear'),\n",
    "    tf.keras.layers.Reshape(target_shape=(-1, 14, 2))\n",
    "])\n",
    "\n",
    "#Complete model including Input, Output and RNN layer stacks\n",
    "rnn_seq2seq_complete = tf.keras.Sequential([\n",
    "    input_processing,\n",
    "    rnn_seq2seq,\n",
    "    output_processing\n",
    "])\n",
    "\n",
    "#Model used during training, to avoid calculating scaling on every iteration\n",
    "rnn_seq2seq_training = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 2)),\n",
    "    rnn_seq2seq\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7cc150cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-58832693d493cc04\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-58832693d493cc04\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir tf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7c2ad7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f04172d16c0>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define callback for Tensorboard update\n",
    "current_dir = dir_logs()\n",
    "callback_tensorboard = tf.keras.callbacks.TensorBoard(current_dir, histogram_freq=50)\n",
    "\n",
    "# Define a learning rate schedule\n",
    "# Get number of gradient descent steps in one epoch\n",
    "steps_in_epoch = 0\n",
    "for i in timeseries_train:\n",
    "    steps_in_epoch += 1\n",
    "# select exponential sheduling scaling learning rate down by a factor of 0.9 every 10 epochs\n",
    "schedule_exp = tf.keras.optimizers.schedules.ExponentialDecay(8E-3, 12*steps_in_epoch, 0.9)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=schedule_exp)\n",
    "rnn_seq2seq_training.compile(optimizer=optimizer, loss='mse', metrics=[Custom_Metric(0), Custom_Metric(1)])\n",
    "rnn_seq2seq_training.fit(timeseries_train, validation_data=timeseries_val, \n",
    "                         epochs = 500, callbacks=[callback_tensorboard], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "423e00bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0037 - custom__metric_33: 0.0741 - custom__metric_34: 0.0191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.019101861864328384"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating model on test dataset\n",
    "loss_test, cm_commuter_test, cm_long_distance_test = rnn_seq2seq_training.evaluate(timeseries_test)\n",
    "cm_long_distance_test\n",
    "#print(f'\\nOn the test data, the model achieves as custom metric (RMSE on 14 day prediction): {cm_cummuter_test:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3413288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "9/9 [==============================] - 0s 13ms/step\n",
      "7/7 [==============================] - 0s 6ms/step\n",
      "tf.Tensor(\n",
      "[0.63821864 0.60805494 0.5865427  0.703927   0.7427517  0.75127006\n",
      " 0.7837306  0.76499516 0.7869938  0.78536195 0.79228884 0.7827962\n",
      " 0.7899372  0.80240345], shape=(14,), dtype=float32)\n",
      "7/7 [==============================] - 0s 6ms/step\n",
      "tf.Tensor(\n",
      "[0.74441934 0.8358058  0.80125093 0.75349295 0.7329046  0.73170555\n",
      " 0.7214597  0.7580035  0.7566102  0.6741412  0.6847973  0.7380386\n",
      " 0.77389437 0.7614313 ], shape=(14,), dtype=float32)\n",
      "7/7 [==============================] - 0s 6ms/step\n",
      "tf.Tensor(\n",
      "[1.5469093 1.6020226 1.5785794 1.6956469 1.7125258 1.7338915 1.754523\n",
      " 1.740252  1.7029456 1.6800865 1.6887573 1.6908216 1.6769826 1.7032954], shape=(14,), dtype=float32)\n",
      "7/7 [==============================] - 0s 6ms/step\n",
      "tf.Tensor(\n",
      "[1.1521171 1.2205957 1.1580614 1.2765483 1.253045  1.2647339 1.2954752\n",
      " 1.2995462 1.3228484 1.3064107 1.3099891 1.3146882 1.3097942 1.3084298], shape=(14,), dtype=float32)\n",
      "7/7 [==============================] - 0s 5ms/step\n",
      "tf.Tensor(\n",
      "[0.95098263 1.0702331  1.0580924  1.0400943  1.0302141  0.99703133\n",
      " 0.9942888  1.0394493  1.0087981  1.0771745  1.0501808  1.0629119\n",
      " 1.0392264  1.0647553 ], shape=(14,), dtype=float32)\n",
      "7/7 [==============================] - 0s 6ms/step\n",
      "tf.Tensor(\n",
      "[1.3884495 1.4804518 1.5782862 1.6209279 1.6340289 1.6445463 1.7056016\n",
      " 1.6971353 1.7166108 1.7348095 1.9084496 1.9072205 1.9126765 1.8982061], shape=(14,), dtype=float32)\n",
      "7/7 [==============================] - 0s 5ms/step\n",
      "tf.Tensor(\n",
      "[0.82402635 0.87828434 0.89901245 0.904381   0.88669336 0.908112\n",
      " 0.88421506 0.93501514 0.9441224  0.95386785 0.7409283  0.710135\n",
      " 0.7108581  0.7167445 ], shape=(14,), dtype=float32)\n",
      "7/7 [==============================] - 0s 5ms/step\n",
      "tf.Tensor(\n",
      "[1.8229656 1.8530303 1.9791495 1.9736748 1.9956074 2.00212   2.005256\n",
      " 2.0082035 2.0485132 2.0289369 2.0621488 2.1147823 2.1566978 2.1688318], shape=(14,), dtype=float32)\n",
      "6/6 [==============================] - 0s 6ms/step\n",
      "tf.Tensor(\n",
      "[0.11895361 0.15645841 0.17546234 0.18058664 0.19243315 0.19323732\n",
      " 0.4304977  0.7428032  0.77967376 0.80217856 0.7940443  0.8021089\n",
      " 0.81129175 0.7971051 ], shape=(14,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Predicting with single sequence\n",
    "rnn_seq2seq_complete.predict(df['commuter'][-70:-14].values.reshape(1, 56, 1))[0, -1, :]\n",
    "rnn_seq2seq_training.predict(commuter_val)[:, -1, 0].shape\n",
    "sum_of_squares = []\n",
    "# TO DO create empty tensors and calculate RMSE for every predicted day sperately\n",
    "for i, (seq, target) in enumerate(commuter_val):\n",
    "    print(tf.reduce_sum(tf.square(target[:, -1, :] - rnn_seq2seq_training.predict(seq)[:, -1, :]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "311c03d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2)\n",
      "(30, 14, 2)\n"
     ]
    }
   ],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices(df[['commuter', 'long_distance']].values)\n",
    "data = data.window(15, shift=1, drop_remainder=True)\n",
    "data = data.flat_map(lambda x: x)\n",
    "data = data.batch(15)\n",
    "data = data.window(30, shift=1, drop_remainder=True)\n",
    "data = data.flat_map(lambda x: x)\n",
    "data = data.batch(30)\n",
    "data = data.map(lambda x: (x[:, 0], x[:, 1:]))\n",
    "for i, item in enumerate(data):\n",
    "    if i < 1:\n",
    "        print(item[0].shape)\n",
    "        print(item[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ac632e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_seq2seq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_seq2seq/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_seq2seq_complete/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_seq2seq_complete/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_seq2seq_training/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_seq2seq_training/assets\n"
     ]
    }
   ],
   "source": [
    "# Saving model\n",
    "\n",
    "# Make sure, that every model is compiled for saving:\n",
    "rnn_seq2seq.compile(optimizer=optimizer, loss='mse', metrics=[Custom_Metric()])\n",
    "rnn_seq2seq_complete.compile(optimizer=optimizer, loss='mse', metrics=[Custom_Metric()])\n",
    "rnn_seq2seq_training.compile(optimizer=optimizer, loss='mse', metrics=[Custom_Metric()])\n",
    "\n",
    "rnn_seq2seq.save('rnn_seq2seq', save_format='tf')\n",
    "rnn_seq2seq_complete.save('rnn_seq2seq_complete', save_format='tf')\n",
    "rnn_seq2seq_training.save('rnn_seq2seq_training', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0f0a42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model2 = tf.keras.models.load_model('rnn_seq2seq_training', custom_objects={'Custom_Metric': Custom_Metric})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4db22e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 15ms/step - loss: 0.0071 - custom__metric_23: 0.0825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00707469554618001, 0.08247807621955872]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(commuter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2ee27c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(32, 56, 2)\n",
      "(18, 56, 2)\n"
     ]
    }
   ],
   "source": [
    "for item in timeseries_train:\n",
    "    print(item[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e215421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "493/493 [==============================] - 3s 2ms/step - loss: 0.2131\n",
      "Epoch 2/10\n",
      "493/493 [==============================] - 1s 2ms/step - loss: 0.1121\n",
      "Epoch 3/10\n",
      "493/493 [==============================] - 1s 2ms/step - loss: 0.0919\n",
      "Epoch 4/10\n",
      "493/493 [==============================] - 1s 2ms/step - loss: 0.0881\n",
      "Epoch 5/10\n",
      "493/493 [==============================] - 1s 2ms/step - loss: 0.0871\n",
      "Epoch 6/10\n",
      "493/493 [==============================] - 1s 2ms/step - loss: 0.0866\n",
      "Epoch 7/10\n",
      "493/493 [==============================] - 1s 2ms/step - loss: 0.0862\n",
      "Epoch 8/10\n",
      "493/493 [==============================] - 1s 2ms/step - loss: 0.0859\n",
      "Epoch 9/10\n",
      "493/493 [==============================] - 1s 2ms/step - loss: 0.0856\n",
      "Epoch 10/10\n",
      "493/493 [==============================] - 1s 2ms/step - loss: 0.0854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f83c8841f60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.LSTM(3, return_sequences=True)\n",
    "])\n",
    "test_rnn.compile(loss='mse', optimizer='adam')\n",
    "x_training_data = np.random.rand(500, 1)\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_training_data)\n",
    "x_train = timeseries_dataset_seq2seq(x_train)\n",
    "y_training_data = np.random.rand(500, 3)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_training_data)\n",
    "\n",
    "\n",
    "test_rnn.fit(x=x_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bf8dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 60, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rnn.predict(np.random.rand(1, 10, 1))\n",
    "np.random.rand(1, 60, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "25804b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67388077 0.18984745 0.61122116 0.31137044 0.50503867 0.18929037\n",
      " 0.61636287 0.97836272 0.70692446 0.79188471 0.11336993 0.01475838\n",
      " 0.83786145 0.50861334]\n",
      "[0.81867838 0.79164872 0.11088158 0.29023907 0.57815831 0.0342671\n",
      " 0.00434693 0.2741532  0.15625099 0.78368318 0.18649465 0.0013391\n",
      " 0.85996041 0.29445972]\n",
      "[0.01330934 0.24549398 0.08873156 0.71743076 0.21148537 0.72991601\n",
      " 0.66889605 0.83391747 0.81458398 0.35312731 0.35237431 0.73738273\n",
      " 0.96517253 0.52211691]\n",
      "[0.08045153 0.92695746 0.30384688 0.22169256 0.85231981 0.36898274\n",
      " 0.69632564 0.17785737 0.99774497 0.93419654 0.98740287 0.81108123\n",
      " 0.90829649 0.51305405]\n",
      "[0.23834562 0.90531048 0.00863958 0.08093868 0.22606262 0.85072973\n",
      " 0.22908413 0.73069026 0.762691   0.32499139 0.65763599 0.64558172\n",
      " 0.69650676 0.10202842]\n",
      "[0.60028532 0.07613098 0.38184849 0.11085824 0.82751313 0.58460833\n",
      " 0.13727838 0.87527711 0.51458954 0.46814201 0.28440379 0.12228945\n",
      " 0.7698344  0.23207313]\n",
      "[0.32205817 0.39806014 0.68102314 0.28397828 0.01403007 0.12466031\n",
      " 0.54509912 0.32188899 0.5034985  0.07103297 0.02632227 0.62627549\n",
      " 0.84521944 0.96162737]\n",
      "[0.44964945 0.96535731 0.87818486 0.18301416 0.01080452 0.61278645\n",
      " 0.3238817  0.61165894 0.64247133 0.05489187 0.58672332 0.8481531\n",
      " 0.56085074 0.71518759]\n",
      "[0.81144944 0.81255406 0.76585015 0.0167234  0.00834109 0.75603269\n",
      " 0.86140576 0.06015929 0.40342788 0.72729481 0.03530758 0.94190276\n",
      " 0.67229717 0.57643199]\n",
      "[0.64400356 0.13655128 0.8332986  0.0185449  0.26777481 0.27449127\n",
      " 0.28898543 0.72502415 0.09113105 0.92011646 0.67805378 0.75153067\n",
      " 0.29665466 0.10618037]\n",
      "[0.47346333 0.44613342 0.6044283  0.79513136 0.33243857 0.98734486\n",
      " 0.66060642 0.93425082 0.4561239  0.59600395 0.69138945 0.68268566\n",
      " 0.04714791 0.34924071]\n",
      "[0.54915037 0.82230529 0.64303598 0.73797224 0.09874984 0.06138457\n",
      " 0.2170812  0.61995817 0.52351431 0.83639597 0.25606677 0.94474034\n",
      " 0.00423285 0.36990968]\n",
      "[0.62916207 0.70845953 0.83946539 0.7509079  0.4371796  0.37972269\n",
      " 0.35942954 0.41287582 0.80661376 0.92173137 0.05422665 0.37877763\n",
      " 0.02521917 0.61325941]\n",
      "[0.02576389 0.88670677 0.54250339 0.10641117 0.34866735 0.76156822\n",
      " 0.30287355 0.17971638 0.04822514 0.28855145 0.30267054 0.5301505\n",
      " 0.13835105 0.55968984]\n",
      "[0.33798793 0.94066733 0.91183926 0.83931707 0.12657836 0.65554698\n",
      " 0.73708292 0.53919446 0.38122131 0.12838358 0.08115912 0.04670611\n",
      " 0.26665036 0.7523315 ]\n",
      "[0.71751975 0.44699908 0.4507161  0.02535093 0.53020273 0.82665169\n",
      " 0.39056484 0.04085058 0.44936695 0.11198516 0.59320776 0.64680055\n",
      " 0.48264001 0.09088077]\n",
      "[0.39052736 0.03771287 0.13764596 0.05873553 0.00851029 0.28863883\n",
      " 0.34525324 0.98827033 0.57798653 0.59622998 0.91635766 0.88961131\n",
      " 0.62788677 0.03113455]\n",
      "[0.83743507 0.87310166 0.21002915 0.02451076 0.87502626 0.09527334\n",
      " 0.80994109 0.46896117 0.43009102 0.06232755 0.62440147 0.96058492\n",
      " 0.40201056 0.10517127]\n",
      "[0.77084595 0.84287741 0.4210406  0.00367403 0.16589142 0.03233794\n",
      " 0.36606983 0.12638273 0.39014338 0.3828352  0.26918524 0.46302582\n",
      " 0.6690314  0.09478815]\n",
      "[0.44857359 0.54856402 0.9969665  0.02840385 0.61707052 0.9906487\n",
      " 0.08636966 0.93316015 0.37327439 0.03458469 0.96864453 0.45429053\n",
      " 0.66135642 0.82112224]\n",
      "[0.88896954 0.93742543 0.72018904 0.91407235 0.90929593 0.59886644\n",
      " 0.51157176 0.93264991 0.72604191 0.68102092 0.22363341 0.35087806\n",
      " 0.43813092 0.0841    ]\n",
      "[0.52491029 0.32967699 0.09620995 0.67936254 0.10820112 0.75912062\n",
      " 0.16849995 0.55112123 0.47437068 0.36422081 0.02548628 0.90185522\n",
      " 0.3896367  0.47442485]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([11.24642  , 13.268541 , 11.237595 ,  7.1986403,  8.0593405,\n",
       "       10.96287  ,  9.327009 , 12.3163805, 11.230288 , 10.433632 ,\n",
       "        8.914517 , 12.7504015, 11.564948 ,  8.877827 ], dtype=float32)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros(14, dtype=np.float32)\n",
    "\n",
    "for i in range(22):\n",
    "    b = np.random.rand(14)\n",
    "    a += b\n",
    "    print(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae57b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This blocks evaluates all possible keys in the nested dictionary \"wagon\" in compositions of one day\n",
    "\n",
    "properties_dict = dict()\n",
    "for train in k.json():\n",
    "    for journey in (train['journeySections']):\n",
    "        for wagon in journey['wagons']:\n",
    "            for i, prop in enumerate(wagon.keys()):\n",
    "                try:\n",
    "                    properties_dict[prop]\n",
    "                except:\n",
    "                    properties_dict[prop] = prop\n",
    "print(properties_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6376e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84644132",
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "session.add(bsp)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e2ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'w') as w:\n",
    "    w.write('haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c57c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
